{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data handling functions\n",
    "def load_iris() -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Load the Iris dataset and return the features and labels\"\"\"\n",
    "    data = pd.read_csv(\"iris.csv\")\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = pd.get_dummies(data.iloc[:, -1]).values\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def load_and_normalize_wine_quality() -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Load the Wine Quality dataset and normalize the features\"\"\"\n",
    "    data = pd.read_csv(\"winequality-white.csv\", delimiter=';')\n",
    "    normalized_data = (data.iloc[:, :-1] - data.iloc[:, :-1].min()) / (data.iloc[:, :-1].max() - data.iloc[:, :-1].min())\n",
    "    y = pd.get_dummies(data.iloc[:, -1]).values\n",
    "    return normalized_data.values, y\n",
    "\n",
    "\n",
    "def custom_train_test_split(X: np.ndarray, y: np.ndarray, test_size=0.2, random_state=42) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Custom train test split function to split the data into training and testing sets\"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    indices = np.random.permutation(X.shape[0])\n",
    "    test_size = int(X.shape[0] * test_size)\n",
    "    test_indices = indices[:test_size]\n",
    "    train_indices = indices[test_size:]\n",
    "    return X[train_indices], X[test_indices], y[train_indices], y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the activation functions\n",
    "# Define activation functions\n",
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    x = np.clip(x, -500, 500)  # Clip x to prevent overflow\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Derivative of the sigmoid activation function\"\"\"\n",
    "    sig = sigmoid(x)\n",
    "    return sig * (1 - sig)\n",
    "\n",
    "\n",
    "def tanh(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Hyperbolic tangent activation function\"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def tanh_derivative(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Derivative of the hyperbolic tangent activation function\"\"\"\n",
    "    return 1 - np.tanh(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP class\n",
    "class MLP:\n",
    "    \"\"\"Multi-Layer Perceptron class\"\"\"\n",
    "    def __init__(self, layer_sizes, activation_function, learning_rate, momentum, batch_size, epochs):\n",
    "        \"\"\"Initialize the Multi-Layer Perceptron\n",
    "        Args:\n",
    "        - layer_sizes: List containing the number of neurons in each layer\n",
    "        - activation_function: Activation function to be used in the hidden layers (sigmoid or tanh)\n",
    "        - learning_rate: Learning rate for the optimization algorithm\n",
    "        - momentum: Momentum for the optimization algorithm\n",
    "        - batch_size: Number of samples to be used in each batch\n",
    "        - epochs: Number of epochs to train the model\n",
    "        \"\"\"\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.activation_function = activation_function\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.train_errors = []\n",
    "        self.test_errors = []\n",
    "        self.initialize_weights_biases()\n",
    "\n",
    "    def initialize_weights_biases(self):\n",
    "        \"\"\"Initialize the weights and biases for the network\"\"\"\n",
    "        for i in range(len(self.layer_sizes) - 1):\n",
    "            limit = np.sqrt(6 / (self.layer_sizes[i] + self.layer_sizes[i + 1]))  # Xavier initialization\n",
    "            self.weights.append(np.random.uniform(-limit, limit, (self.layer_sizes[i], self.layer_sizes[i + 1])))\n",
    "            self.biases.append(np.zeros((1, self.layer_sizes[i + 1])))\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> list:\n",
    "        \"\"\"Forward pass through the network to compute the activations\"\"\"\n",
    "        activations = [X]\n",
    "        for i in range(len(self.weights)):\n",
    "            activation_function = sigmoid if self.activation_function == \"sigmoid\" else tanh\n",
    "            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "            a = activation_function(z)\n",
    "            activations.append(a)\n",
    "        return activations\n",
    "\n",
    "    def backpropagation(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"Backpropagation algorithm to update the weights and biases\"\"\"\n",
    "        activations = self.forward(X)\n",
    "        delta = (activations[-1] - y)\n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            derivative_function = sigmoid_derivative if self.activation_function == \"sigmoid\" else tanh_derivative\n",
    "            delta *= derivative_function(activations[i + 1])\n",
    "            gradient_w = np.dot(activations[i].T, delta)\n",
    "            gradient_b = np.sum(delta, axis=0, keepdims=True)\n",
    "            self.weights[i] -= self.learning_rate * gradient_w\n",
    "            self.biases[i] -= self.learning_rate * gradient_b\n",
    "            if i != 0:\n",
    "                delta = np.dot(delta, self.weights[i].T)\n",
    "\n",
    "    def compute_loss(self, y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "        \"\"\"Compute the mean squared error loss\"\"\"\n",
    "        return np.mean((y_pred - y_true) ** 2)\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predict the output for the given input\"\"\"\n",
    "        layer_outputs = self.forward(X)\n",
    "        return layer_outputs[-1]\n",
    "\n",
    "    def evaluate(self, X: np.ndarray, y: np.ndarray) -> Tuple[float, float]:\n",
    "        predictions = self.predict(X)\n",
    "        accuracy = np.mean(np.argmax(predictions, axis=1) == np.argmax(y, axis=1))\n",
    "        return accuracy\n",
    "\n",
    "    def fit(self, X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, y_test: np.ndarray):\n",
    "        \"\"\"Train the model using the training data and validate it using the test data\"\"\"\n",
    "        for epoch in range(self.epochs):\n",
    "            indices = np.arange(len(X_train))\n",
    "            np.random.shuffle(indices)\n",
    "            X_train = X_train[indices]\n",
    "            y_train = y_train[indices]\n",
    "\n",
    "            for i in range(0, len(X_train), self.batch_size):\n",
    "                X_batch = X_train[i:i + self.batch_size]\n",
    "                y_batch = y_train[i:i + self.batch_size]\n",
    "                self.backpropagation(X_batch, y_batch)\n",
    "\n",
    "            train_pred = self.forward(X_train)[-1]\n",
    "            test_pred = self.forward(X_test)[-1]\n",
    "            train_loss = self.compute_loss(train_pred, y_train)\n",
    "            test_loss = self.compute_loss(test_pred, y_test)\n",
    "            self.train_errors.append(train_loss)\n",
    "            self.test_errors.append(test_loss)\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{self.epochs} - Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    def plot_errors(self):\n",
    "        \"\"\"Plot the train and test errors vs epochs\"\"\"\n",
    "        plt.plot(self.train_errors, label=\"Train Error\")\n",
    "        plt.plot(self.test_errors, label=\"Test Error\")\n",
    "        plt.title(\"Train and Test Errors vs Epochs\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Error\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get valid inputs from user\n",
    "def get_inputs_from_user() -> Tuple[List[int], str, float, float, int, int]:\n",
    "    \"\"\"This function gets the hyperparameters from the user\"\"\"\n",
    "    # Get the number of layers and based on the number of layers get the number of neurons in each layer\n",
    "    layer_sizes = []\n",
    "    while True:\n",
    "        num_layers = input(\"Enter the number of hidden layers: \")\n",
    "        if num_layers and num_layers.isnumeric() and int(num_layers) > 0:\n",
    "            for i in range(int(num_layers)):\n",
    "                while True:\n",
    "                    num_neurons = input(f\"Enter the number of neurons in layer {i + 1}: \")\n",
    "                    if num_neurons and num_neurons.isnumeric() and int(num_neurons) > 0:\n",
    "                        layer_sizes.append(int(num_neurons))\n",
    "                        break\n",
    "                    else:\n",
    "                        print(\"Please enter a valid positive number.\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Please enter a valid positive number.\")\n",
    "    \n",
    "    # Get the activation function\n",
    "    activation_function = input(\"Enter the activation function (sigmoid/tanh): \")\n",
    "    while activation_function not in [\"sigmoid\", \"tanh\"]:\n",
    "        activation_function = input(\"Please enter a valid activation function (sigmoid/tanh): \")\n",
    "\n",
    "    # Get the learning rate\n",
    "    learning_rate = input(\"Enter the learning rate: \")\n",
    "    while not learning_rate.replace(\".\", \"\", 1).isdigit():\n",
    "        learning_rate = input(\"Please enter a valid learning rate: \")\n",
    "    \n",
    "    # Get the momentum\n",
    "    momentum = input(\"Enter the momentum: \")\n",
    "    while not momentum.replace(\".\", \"\", 1).isdigit():\n",
    "        momentum = input(\"Please enter a valid momentum: \")\n",
    "\n",
    "    # Get the batch size\n",
    "    batch_size = input(\"Enter the batch size: \")\n",
    "    while not batch_size.isnumeric() or int(batch_size) <= 0:\n",
    "        batch_size = input(\"Please enter a valid batch size: \")\n",
    "\n",
    "    # Get the number of epochs\n",
    "    epochs = input(\"Enter the number of epochs: \")\n",
    "    while not epochs.isnumeric() or int(epochs) <= 0:\n",
    "        epochs = input(\"Please enter a valid number of epochs: \")\n",
    "\n",
    "    return layer_sizes, activation_function, float(learning_rate), float(momentum), int(batch_size), int(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the hyperparameters from the user\n",
    "layer_sizes, activation_function, learning_rate, momentum, batch_size, epochs = get_inputs_from_user()\n",
    "# Adjust the layer sizes based on the input for both datasets\n",
    "layer_sizes_iris = [4] + layer_sizes + [3]\n",
    "layer_sizes_wine = [11] + layer_sizes + [7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split data for Iris dataset\n",
    "X_iris, y_iris = load_iris()\n",
    "print('############ Iris Dataset ############')\n",
    "print(X_iris.shape, y_iris.shape)\n",
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris = custom_train_test_split(X_iris, y_iris)\n",
    "print(X_train_iris.shape, y_train_iris.shape)\n",
    "print(X_test_iris.shape, y_test_iris.shape)\n",
    "\n",
    "# Load and split data for Wine Quality dataset\n",
    "X_wine, y_wine = load_and_normalize_wine_quality()\n",
    "print('############ Wine Quality Dataset ############')\n",
    "print(X_wine.shape, y_wine.shape)\n",
    "X_train_wine, X_test_wine, y_train_wine, y_test_wine = custom_train_test_split(X_wine, y_wine)\n",
    "print(X_train_wine.shape, y_train_wine.shape)\n",
    "print(X_test_wine.shape, y_test_wine.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the MLP class for Iris dataset\n",
    "mlp_iris = MLP(layer_sizes_iris, activation_function, learning_rate, momentum, batch_size, epochs)\n",
    "# Train the model for Iris dataset\n",
    "mlp_iris.fit(X_train_iris, y_train_iris, X_test_iris, y_test_iris)\n",
    "# Evaluate the model for Iris dataset\n",
    "train_accuracy_iris = mlp_iris.evaluate(X_train_iris, y_train_iris)\n",
    "test_accuracy_iris = mlp_iris.evaluate(X_test_iris, y_test_iris)\n",
    "mlp_iris.plot_errors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the MLP class for Wine Quality dataset\n",
    "mlp_wine = MLP(layer_sizes_wine, activation_function, learning_rate, momentum, batch_size, epochs)\n",
    "# Train the model for Wine Quality dataset\n",
    "mlp_wine.fit(X_train_wine, y_train_wine, X_test_wine, y_test_wine)\n",
    "# Evaluate the model for Wine Quality dataset\n",
    "train_accuracy_wine = mlp_wine.evaluate(X_train_wine, y_train_wine)\n",
    "test_accuracy_wine = mlp_wine.evaluate(X_test_wine, y_test_wine)\n",
    "mlp_wine.plot_errors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results for Iris dataset\n",
    "print(\"Iris Dataset:\")\n",
    "print(f\"Train Accuracy: {train_accuracy_iris:.2f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy_iris:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results for Wine Quality dataset\n",
    "print(\"\\nWine Quality Dataset:\")\n",
    "print(f\"Train Accuracy: {train_accuracy_wine:.2f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy_wine:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
